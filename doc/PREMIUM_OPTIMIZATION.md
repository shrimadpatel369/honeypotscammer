# Premium Optimization Guide

This guide covers the premium optimizations implemented for minimum latency and maximum performance.

## ðŸš€ Performance Optimizations

### 1. Premium Gemini Model
- **Model**: `gemini-2.0-flash-thinking-exp` (latest premium model)
- **Features**:
  - Advanced reasoning capabilities
  - Faster response times
  - Higher accuracy for scam detection
  - Better conversation quality
  - Optimized generation config for each task

### 2. MongoDB Cloud Optimization
```python
# Connection pool settings
maxPoolSize=100
minPoolSize=10
maxIdleTimeMS=45000
retryWrites=True
retryReads=True
w='majority'
compressors='snappy,zlib'  # Network compression
```

**Benefits**:
- Up to 100 concurrent connections
- Connection reuse for faster queries
- Automatic retry for transient failures
- Data compression reduces network latency
- Write acknowledgment ensures data consistency

### 3. In-Memory Caching
- **Implementation**: High-performance LRU cache
- **Features**:
  - Scam detection pattern caching
  - 300-second TTL (configurable)
  - Thread-safe async operations
  - Cache hit/miss statistics
  - Automatic expiration

**Performance Impact**:
- Cache Hit: <1ms response time
- Cache Miss: ~200-500ms (Gemini API call)
- Expected hit rate: 40-60% for repeated patterns

### 4. Rate Limiting
- **Default**: 100 requests per minute per IP
- **Implementation**: SlowAPI with Redis-like memory backend
- **Benefits**:
  - Prevents abuse
  - Ensures fair resource allocation
  - Protects against DDoS

### 5. Async Optimization
- **Event Loop**: uvloop (2x faster than asyncio)
- **HTTP**: httptools (faster HTTP parsing)
- **Workers**: 4 workers for parallel request handling
- **Non-blocking I/O**: All database and API calls are async

### 6. Request Processing Pipeline

```
Request â†’ Rate Limit â†’ Auth â†’ Cache Check
   â†“
Cache Hit? â†’ Return Cached Result (1ms)
   â†“
Cache Miss â†’ Gemini API (200-500ms)
   â†“
MongoDB Save â†’ Return Response
```

## ðŸ“Š Latency Breakdown

### Expected Response Times

| Operation | Latency | Notes |
|-----------|---------|-------|
| Cache Hit | 1-5ms | Memory lookup only |
| Database Query | 5-20ms | Cloud MongoDB with indexes |
| Gemini API (Scam Detection) | 200-500ms | Premium model |
| Gemini API (AI Agent) | 300-700ms | Conversation generation |
| Full Request (Cached) | 50-100ms | With cache hit |
| Full Request (Uncached) | 500-1500ms | First request or unique pattern |

### Cloud Run Optimization

**Recommended Settings**:
```yaml
Memory: 2 GiB
CPU: 2
Max Instances: 100
Min Instances: 1  # Reduces cold starts
Concurrency: 80
Timeout: 300s
```

**Cold Start**: ~2-4 seconds (first request)
**Warm Start**: 50-1500ms (subsequent requests)

## ðŸŽ¯ MongoDB Atlas Optimization

### Index Strategy
```javascript
// Compound indexes for fast queries
db.sessions.createIndex({ "sessionId": 1 }, { unique: true })
db.sessions.createIndex({ "startTime": -1 })
db.sessions.createIndex({ "status": 1, "scamDetected": 1 })
```

### Connection String
```
mongodb+srv://user:pass@cluster.mongodb.net/?retryWrites=true&w=majority&maxPoolSize=100
```

### Atlas Tier Recommendations
- **M10+**: For production workload
- **M20+**: For high traffic (1000+ requests/min)
- **Features**: Auto-scaling, point-in-time recovery, advanced monitoring

## ðŸ”§ Environment Variables for Premium Setup

```env
# Gemini Premium
GEMINI_API_KEY=your-premium-key
GEMINI_MODEL=gemini-2.0-flash-thinking-exp
GEMINI_MAX_RETRIES=3
GEMINI_TIMEOUT=30

# MongoDB Cloud (Atlas)
MONGODB_URL=mongodb+srv://user:pass@cluster.mongodb.net/
MONGODB_DB_NAME=honeypot_db
MONGODB_MAX_POOL_SIZE=100
MONGODB_MIN_POOL_SIZE=10

# Performance
MAX_CONNECTIONS=100
ENABLE_CACHING=True
CACHE_TTL=300
WORKERS=4

# Rate Limiting
RATE_LIMIT_PER_MINUTE=100
```

## ðŸ“ˆ Monitoring & Metrics

### Cache Statistics
```bash
curl http://your-api/health
```

Response includes:
```json
{
  "cache": {
    "size": 450,
    "max_size": 1000,
    "hits": 1250,
    "misses": 800,
    "hit_rate": "61.00%"
  }
}
```

### Request Metrics
Check `X-Process-Time` header in responses:
```
X-Process-Time: 0.342
```

### Cloud Run Metrics
- Request count
- Request latency (p50, p95, p99)
- Instance count
- CPU utilization
- Memory utilization

## ðŸš€ Cloud Run Premium Deployment

### Step 1: Push to GitHub
```bash
git add .
git commit -m "Premium optimization"
git push origin main
```

### Step 2: Cloud Run Configuration

**Container Settings**:
- Memory: **2 GiB** (minimum for premium performance)
- CPU: **2** (faster processing)
- Max instances: **100** (auto-scale to demand)
- Min instances: **1** (keep warm instance)
- Concurrency: **80** (requests per instance)

**Environment Variables**:
```
API_KEY=<secret>
GEMINI_API_KEY=<secret>
GEMINI_MODEL=gemini-2.0-flash-thinking-exp
MONGODB_URL=<atlas-connection-string>
MONGODB_MAX_POOL_SIZE=100
ENABLE_CACHING=True
WORKERS=4
RATE_LIMIT_PER_MINUTE=100
```

### Step 3: Enable Performance Features

```bash
gcloud run deploy honeypot-api \
  --image gcr.io/PROJECT/honeypot-api \
  --platform managed \
  --region us-central1 \
  --memory 2Gi \
  --cpu 2 \
  --concurrency 80 \
  --max-instances 100 \
  --min-instances 1 \
  --timeout 300 \
  --execution-environment gen2 \
  --cpu-boost \
  --set-env-vars="WORKERS=4,ENABLE_CACHING=True"
```

**Key Flags**:
- `--execution-environment gen2`: Latest runtime (faster)
- `--cpu-boost`: CPU boost during startup
- `--min-instances 1`: Always warm instance

## ðŸŽ¨ Load Testing

### Using Apache Bench
```bash
# Test endpoint performance
ab -n 1000 -c 10 -H "x-api-key: YOUR_KEY" \
   -p test.json -T application/json \
   https://your-api/api/v1/honeypot
```

### Expected Results
- **Requests per second**: 50-100 (with caching)
- **Mean response time**: 100-500ms
- **95th percentile**: <1000ms
- **Failed requests**: 0%

## ðŸ’° Cost Estimation (Premium)

### Google Cloud
- **Cloud Run**: $0.024/hour (2GB, 2 CPU) Ã— 24 Ã— 30 = $17.28/month
- **Networking**: ~$5/month
- **Total**: ~$22-25/month

### MongoDB Atlas
- **M10 Tier**: $57/month (suitable for production)
- **M20 Tier**: $164/month (high traffic)

### Gemini API
- **Free Tier**: 1M tokens/month
- **Paid**: $0.00025/1K tokens (input), $0.00075/1K tokens (output)
- **Estimated**: $10-30/month (moderate usage)

### Total Monthly Cost
- **Basic Production**: ~$90-115/month
- **High Traffic**: ~$200-250/month

## ðŸ” Troubleshooting Performance Issues

### High Latency
1. **Check cache hit rate** (should be >50%)
2. **Monitor Gemini API response time**
3. **Check MongoDB connection pool usage**
4. **Increase Cloud Run memory/CPU**
5. **Enable min instances to reduce cold starts**

### Memory Issues
1. Reduce cache size in `app/cache.py`
2. Reduce MongoDB pool size
3. Increase Cloud Run memory allocation

### Connection Pool Exhaustion
1. Increase `MONGODB_MAX_POOL_SIZE`
2. Reduce request timeout
3. Scale horizontally (more instances)

## ðŸ“š Best Practices

1. **Always use caching** for production
2. **Monitor cache hit rate** and adjust TTL
3. **Keep min instances = 1** to avoid cold starts
4. **Use MongoDB Atlas M10+** for production
5. **Enable Cloud Run gen2** for better performance
6. **Set up alerts** for high latency/errors
7. **Regular load testing** to validate performance
8. **Use compression** for large responses

## ðŸŽ¯ Performance Tuning Checklist

- [ ] MongoDB connection pool optimized (100 max)
- [ ] Caching enabled with appropriate TTL
- [ ] Premium Gemini model configured
- [ ] Cloud Run with 2GB+ memory
- [ ] Min instances = 1 for warm starts
- [ ] Indexes created on MongoDB
- [ ] Rate limiting configured
- [ ] Multiple workers (4+)
- [ ] uvloop enabled for async
- [ ] Monitoring and alerts set up

---

With these optimizations, the system achieves:
- âœ… **Sub-second response times** (with caching)
- âœ… **High throughput** (50-100 req/s per instance)
- âœ… **Auto-scaling** (1-100 instances)
- âœ… **99.9% uptime** with Cloud Run
- âœ… **Production-grade reliability**
